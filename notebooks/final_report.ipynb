{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e63556d3",
   "metadata": {},
   "source": [
    "# Expression-Preserving Face Anonymization — Final Report\n",
    "\n",
    "**COMP4026 Final-Year Project — HKBU**\n",
    "\n",
    "This notebook loads all experiment results, produces summary tables, and\n",
    "generates privacy–utility Pareto frontier visualisations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f489f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({\"figure.dpi\": 120, \"font.size\": 10})\n",
    "\n",
    "# Ensure project root is on the path\n",
    "ROOT = Path(os.getcwd()).parent\n",
    "RESULTS = ROOT / \"results\"\n",
    "print(f\"Project root: {ROOT}\")\n",
    "print(f\"Results dir:  {RESULTS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5802bf9",
   "metadata": {},
   "source": [
    "## 1 — Baseline Sweep (Classical Anonymizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d1a46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_csv = RESULTS / \"baseline_sweep\" / \"frontier.csv\"\n",
    "df_baseline = pd.read_csv(baseline_csv)\n",
    "cols = [\"anonymizer\", \"params\", \"closed_set_top1\", \"privacy_score\",\n",
    "        \"acc_anonymized\", \"expr_consistency\", \"utility_score\",\n",
    "        \"lpips_mean\", \"psnr_mean\", \"ssim_mean\"]\n",
    "df_baseline[cols].sort_values(\"privacy_score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12865f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "for anon in df_baseline[\"anonymizer\"].unique():\n",
    "    sub = df_baseline[df_baseline[\"anonymizer\"] == anon]\n",
    "    ax.scatter(sub[\"privacy_score\"], sub[\"utility_score\"], label=anon, s=60)\n",
    "ax.set_xlabel(\"Privacy Score (higher = more private)\")\n",
    "ax.set_ylabel(\"Utility Score (higher = better expression)\")\n",
    "ax.set_title(\"Baseline Sweep — Privacy vs Utility Frontier\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e1e92b",
   "metadata": {},
   "source": [
    "## 2 — k-Same Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcbd03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ksame_csv = RESULTS / \"ksame_sweep\" / \"frontier.csv\"\n",
    "df_ksame = pd.read_csv(ksame_csv)\n",
    "df_ksame[cols].sort_values(\"privacy_score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dbe196",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4.5))\n",
    "\n",
    "# Extract k from params JSON\n",
    "df_ksame[\"k\"] = df_ksame[\"params\"].apply(lambda p: json.loads(p).get(\"k\", 0))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(df_ksame[\"k\"], df_ksame[\"closed_set_top1\"], \"o-\", color=\"tab:red\")\n",
    "ax.set_xlabel(\"k\")\n",
    "ax.set_ylabel(\"Closed-set Top-1 (lower = more private)\")\n",
    "ax.set_title(\"k-Same: Identity Leakage vs k\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(df_ksame[\"k\"], df_ksame[\"utility_score\"], \"s-\", color=\"tab:blue\")\n",
    "ax.set_xlabel(\"k\")\n",
    "ax.set_ylabel(\"Utility Score\")\n",
    "ax.set_title(\"k-Same: Expression Utility vs k\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7629b800",
   "metadata": {},
   "source": [
    "## 3 — GAN-Based Anonymiser Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ecfc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_csv = RESULTS / \"gan_comparison\" / \"frontier.csv\"\n",
    "df_gan = pd.read_csv(gan_csv)\n",
    "df_gan[cols].sort_values(\"privacy_score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c234e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "for _, row in df_gan.iterrows():\n",
    "    ax.scatter(row[\"privacy_score\"], row[\"utility_score\"], s=100, zorder=5)\n",
    "    ax.annotate(row[\"anonymizer\"], (row[\"privacy_score\"], row[\"utility_score\"]),\n",
    "                textcoords=\"offset points\", xytext=(8, 4), fontsize=9)\n",
    "\n",
    "# Overlay baseline frontier for context\n",
    "ax.scatter(df_baseline[\"privacy_score\"], df_baseline[\"utility_score\"],\n",
    "           marker=\"x\", alpha=0.4, color=\"gray\", label=\"Baselines\")\n",
    "ax.set_xlabel(\"Privacy Score\")\n",
    "ax.set_ylabel(\"Utility Score\")\n",
    "ax.set_title(\"GAN Anonymisers vs Classical Baselines\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b08c83c",
   "metadata": {},
   "source": [
    "## 4 — Conditioning Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9820c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_csv = RESULTS / \"conditioning_ablation\" / \"frontier.csv\"\n",
    "df_cond = pd.read_csv(cond_csv)\n",
    "df_cond[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca05dca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the ablation markdown table\n",
    "ablation_md = (RESULTS / \"conditioning_ablation\" / \"ablation_table.md\").read_text()\n",
    "from IPython.display import Markdown\n",
    "Markdown(ablation_md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea1b5c4",
   "metadata": {},
   "source": [
    "## 5 — Expression-Loss Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4db6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_csv = RESULTS / \"expression_loss_ablation\" / \"frontier.csv\"\n",
    "df_expr = pd.read_csv(expr_csv)\n",
    "df_expr[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae3f9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation_expr_md = (RESULTS / \"expression_loss_ablation\" / \"ablation_table.md\").read_text()\n",
    "Markdown(ablation_expr_md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4b080b",
   "metadata": {},
   "source": [
    "## 6 — Adaptive Attacker Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482e1e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptive_csv = RESULTS / \"adaptive_attacker_ablation\" / \"comparison.csv\"\n",
    "df_adaptive = pd.read_csv(adaptive_csv)\n",
    "df_adaptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc9eb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptive_md = (RESULTS / \"adaptive_attacker_ablation\" / \"comparison_table.md\").read_text()\n",
    "Markdown(adaptive_md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f5868f",
   "metadata": {},
   "source": [
    "## 7 — Combined Pareto Frontier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96eb0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all frontier CSVs\n",
    "frames = []\n",
    "for label, df in [(\"Baseline\", df_baseline), (\"k-Same\", df_ksame), (\"GAN\", df_gan)]:\n",
    "    tmp = df.copy()\n",
    "    tmp[\"experiment\"] = label\n",
    "    frames.append(tmp)\n",
    "df_all = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "markers = {\"Baseline\": \"o\", \"k-Same\": \"s\", \"GAN\": \"D\"}\n",
    "for exp in [\"Baseline\", \"k-Same\", \"GAN\"]:\n",
    "    sub = df_all[df_all[\"experiment\"] == exp]\n",
    "    ax.scatter(sub[\"privacy_score\"], sub[\"utility_score\"],\n",
    "               marker=markers[exp], s=70, label=exp, alpha=0.8)\n",
    "\n",
    "ax.set_xlabel(\"Privacy Score (higher = more private)\", fontsize=12)\n",
    "ax.set_ylabel(\"Utility Score (higher = better expression)\", fontsize=12)\n",
    "ax.set_title(\"Combined Privacy–Utility Pareto Frontier\", fontsize=13)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(RESULTS / \"combined_frontier.png\"), dpi=150)\n",
    "plt.show()\n",
    "print(\"Saved: results/combined_frontier.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe5a01e",
   "metadata": {},
   "source": [
    "## 8 — Realism Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4389c312",
   "metadata": {},
   "outputs": [],
   "source": [
    "realism_cols = [\"anonymizer\", \"params\", \"lpips_mean\", \"psnr_mean\", \"ssim_mean\"]\n",
    "print(\"=== Classical Baselines ===\")\n",
    "display(df_baseline[realism_cols].sort_values(\"lpips_mean\"))\n",
    "print(\"\\n=== k-Same ===\")\n",
    "display(df_ksame[realism_cols].sort_values(\"lpips_mean\"))\n",
    "print(\"\\n=== GAN Methods ===\")\n",
    "display(df_gan[realism_cols].sort_values(\"lpips_mean\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bff929e",
   "metadata": {},
   "source": [
    "## 9 — Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc0ca8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total experiment runs:  {len(df_all)}\")\n",
    "print(f\"  Baseline sweep:       {len(df_baseline)}\")\n",
    "print(f\"  k-Same sweep:         {len(df_ksame)}\")\n",
    "print(f\"  GAN comparison:       {len(df_gan)}\")\n",
    "print(f\"  Conditioning ablation:{len(df_cond)}\")\n",
    "print(f\"  Expression-loss ablation: {len(df_expr)}\")\n",
    "if 'df_adaptive' in dir():\n",
    "    print(f\"  Adaptive attacker:    {len(df_adaptive)}\")\n",
    "\n",
    "# Best privacy\n",
    "best_priv = df_all.loc[df_all[\"privacy_score\"].idxmax()]\n",
    "print(f\"\\nBest privacy:  {best_priv['anonymizer']} \"\n",
    "      f\"(privacy={best_priv['privacy_score']:.2f}, \"\n",
    "      f\"utility={best_priv['utility_score']:.2f})\")\n",
    "\n",
    "# Best utility\n",
    "best_util = df_all.loc[df_all[\"utility_score\"].idxmax()]\n",
    "print(f\"Best utility:  {best_util['anonymizer']} \"\n",
    "      f\"(privacy={best_util['privacy_score']:.2f}, \"\n",
    "      f\"utility={best_util['utility_score']:.2f})\")\n",
    "\n",
    "# Best trade-off (sum of scores)\n",
    "df_all[\"combined\"] = df_all[\"privacy_score\"] + df_all[\"utility_score\"]\n",
    "best_combo = df_all.loc[df_all[\"combined\"].idxmax()]\n",
    "print(f\"Best trade-off: {best_combo['anonymizer']} \"\n",
    "      f\"(privacy={best_combo['privacy_score']:.2f}, \"\n",
    "      f\"utility={best_combo['utility_score']:.2f}, \"\n",
    "      f\"combined={best_combo['combined']:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea83388",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**End of Report**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
